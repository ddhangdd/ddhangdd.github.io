I"k<p>This section serves as a placeholder for my notes, so that I can review certain concepts when needed.</p>

<p>Key ideas covered:    <a href="L09_regularization__slides 2.pdf" target="_blank">L09_regularization</a></p>

<ul>
  <li>Common Regularization Techniques, e.g. Early stopping, L<sub>1</sub>/L<sub>2</sub> , and dropout</li>
  <li>Validation set is used for model section, for hyperparameter tuning, tells you info about training</li>
  <li>High variance, fit data closely so when you switch data, boundary will have high variance than previous line.</li>
  <li>WHy does dropout work well (Slides 35)</li>
</ul>

<p>Key ideas covered:   <a href="L10_norm-and-init__slides.pdf" target="_blank">L10_norm</a></p>

<ul>
  <li>BatchNorm normalizes hidden layer inputs (slides 7)</li>
  <li>BatchNorm has 2 main steps, standardization and learnable scaling</li>
  <li>Backprop for BatchNorm parameters</li>
  <li>BatchNorm provides additional parameters that will help layers to learn a little bit more independently, BatchNorm allows larger learning rates</li>
  <li>BatchNorm becomes more stable with learger minibatch sizes, since s.d. and mean will be too noisy if we use small minibatch size ( each minibatch has their own mean and s.d.)</li>
  <li>We do normalization when feature have different ranges</li>
  <li>Why minibatch sizes as power of 2</li>
</ul>

<hr />

<hr />
:ET